{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " INFO5731_Assignment_Two_Kolla.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#CovidVaccine\"](https://twitter.com/hashtag/CovidVaccine) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "outputId": "e371a514-fdb6-4634-d975-20426d232559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# Write your code here\n",
        "# question 3 Collect the abstracts of the top 100 research papers by using the query natural language processing from CiteSeerX.\n",
        "%tensorflow_version 2.x\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.14.0\n",
        "# All 100 article names\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "name = [ ]\n",
        "name_List = [ ]\n",
        "abstract = [ ]\n",
        "abstract_List=[ ]\n",
        "page_number = [0,10,20,30,40,50,60,70,80,90]\n",
        "for i in page_number:\n",
        " page = requests.get('https://citeseerx.ist.psu.edu/search;jsessionid=87FF6C66EA09F22314C131669600CF98?q=natural+language+processing&t=doc&sort=rlv&start='+ str (i))\n",
        " soup = BeautifulSoup(page.content,'html.parser')\n",
        " name.append(soup.find_all('a',class_='remove doc_details'))\n",
        " abstract.append(soup.find_all('div',class_='pubabstract'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXDenGZPmFzr"
      },
      "source": [
        "for j in name:\n",
        "  for k in range(len(j)):\n",
        "    name_List.append(j[k].get_text())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aplXSObdmUtp"
      },
      "source": [
        "# All 100 article abstracts\n",
        "for l in abstract:\n",
        "  for m in range(len(l)):\n",
        "    abstract_List.append(l[m].get_text())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-QPH_G3pp3_"
      },
      "source": [
        "df = pd.DataFrame({'Article':name_List,'Abstract': abstract_List})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQk-U_cqMNi"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wntVUCYwE3Nv"
      },
      "source": [
        "df.to_csv('Articles.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "#1 Noise removal\n",
        "df['Article'] = df['Article'].replace('\\n','',regex = True)\n",
        "df['Abstract'] = df['Abstract'].replace('\\n','',regex = True)\n",
        "df['Article'] = df['Article'].replace('[^\\w\\s]','',regex = True)\n",
        "df['Abstract'] = df['Abstract'].replace('[^\\w\\s]','',regex = True)\n",
        "#2 Remove numbers\n",
        "df['Article'] = df['Article'].str.replace('\\d+','')\n",
        "df['Abstract'] = df['Abstract'].str.replace('\\d+','')\n",
        "#3 Remove StopWords\n",
        "stop_Words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \n",
        "              \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\",\n",
        "              \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\",\n",
        "              \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\",\n",
        "              \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "              \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\",\n",
        "              \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n",
        "              \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n",
        "              \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\",\n",
        "              \"about\", \"against\", \"between\", \"into\", \"through\",\n",
        "              \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
        "              \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
        "              \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "              \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
        "              \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
        "              \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "              \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
        "              \"don\", \"should\", \"now\"]\n",
        "# stop = stopwords.words('english')\n",
        "df['Article'] = df['Article'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_Words))\n",
        "df['Abstract'] = df['Abstract'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_Words))\n",
        "#4 Lowercase all text\n",
        "df['Article'] = df['Article'].str.lower()\n",
        "df['Abstract'] = df['Abstract'].str.lower()\n",
        "#5 Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['Article'] = df['Article'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "df['Abstract']= df['Abstract'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "#6 Lemmatization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "df['Abstract']=df['Abstract'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "df.to_csv('Articles.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBkchW2__vGm"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr"
      },
      "source": [
        "# Write your code here\n",
        "#1\n",
        "texts = df['Abstract'].tolist() # convert the column of strings to text.\n",
        "map(word_tokenize, texts)\n",
        "df['Abstract'].apply(word_tokenize)\n",
        "# df['Abstract'].apply(word_tokenize).tolist()\n",
        "# pos_tag_sents( df['Abstract'].apply(word_tokenize).tolist() )\n",
        "df['Abstract_POS'] = pos_tag_sents( df['Abstract'].apply(word_tokenize).tolist() )\n",
        "df['Abstract_POS'].head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSZBUQD_aT4e"
      },
      "source": [
        "#dependency parsing\n",
        "depend_Tree = []\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "df['Abstract_nlp'] = df['Abstract'].apply(lambda x: nlp(x))\n",
        "df['Abstract_sent'] = df['Abstract_nlp'].apply(lambda x: x.sents)\n",
        "df['Abstract_sent'].apply(lambda x: displacy.render(x,style='dep', jupyter=True)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ludn59SRt-9N"
      },
      "source": [
        "# single sentence dependency parsing example\n",
        "text = nlp(df['Abstract'][0])\n",
        "print(displacy.render(text.sents,style='dep', jupyter=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpoc5xXa54OW"
      },
      "source": [
        "!pip install cython numpy\n",
        "!pip install benepar[cpu]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3UuyxdWuT-o"
      },
      "source": [
        "# #2\n",
        "import benepar\n",
        "benepar.download('benepar_en2')\n",
        "import nltk.draw.tree\n",
        "from nltk.tree import *\n",
        "par = benepar.Parser(\"benepar_en2\")\n",
        "df['parseTree'] = df['Abstract'].apply(lambda x: par.parse(i) for i in df['Abstract'])\n",
        "# parseTree = list(map(lambda x:x.replace('\\n',''),parseTree))\n",
        "for i in df['parseTree']:\n",
        "    Tree.fromstring(i).pretty_print()\n",
        "#  print('----------------------------')\n",
        "#  print('----------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3CuUlpI1kqg"
      },
      "source": [
        "dat = 0\n",
        "tim = 0\n",
        "car = 0\n",
        "ord = 0\n",
        "nor = 0\n",
        "org = 0 \n",
        "for i in range(0,100):\n",
        "  text = nlp(df['Abstract'][i]) \n",
        "  for j in text.ents:\n",
        "    if j.label_ == \"DATE\":\n",
        "       dat=dat+1\n",
        "    elif j.label_ == \"TIME\":\n",
        "       tim=tim+1\n",
        "    elif j.label_ == \"CARDINAL\":\n",
        "       car=car+1\n",
        "    elif j.label_ == \"ORDINAL\":\n",
        "       ord=ord+1\n",
        "    elif j.label_ == \"NORP\":\n",
        "       nor=nor+1\n",
        "    elif j.label_ == \"ORG\":\n",
        "       org=org+1\n",
        "    else: 0\n",
        "    print(j.text, j.label_)\n",
        "print('\\n')\n",
        "print('\\n')\n",
        "print(\"DATE:\",dat) \n",
        "print(\"TIME:\",tim) \n",
        "print(\"CARDINAL:\",car) \n",
        "print(\"ORDINAL:\",ord) \n",
        "print(\"NORP:\",nor) \n",
        "print(\"ORG:\",org)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkGtJSFn8i9m"
      },
      "source": [
        "'''\n",
        "Write your explanations of the constituency parsing tree and dependency parsing tree here\n",
        "\n",
        "Constituency parsing is extremely helpful in envisioning the whole syntactical structure of a sentence. These parse trees can be useful in word processing methods for syntax checking.\n",
        "\n",
        "Dependency parsing determines the grammatical structure by listing each word as a node and presenting links to its dependents. A constituency parsed tree displays the syntactic structure of a sentence using a context-free grammar.\n",
        "\n",
        "Taking the second sentence from the General text from the above parsing trees as an example, we see that dependency parsing represents only the connections between words rather than the sentence structure and relationship.\n",
        "\n",
        "Constituency parsing and dependency is necessary for word processing systems and grammar checking.\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}