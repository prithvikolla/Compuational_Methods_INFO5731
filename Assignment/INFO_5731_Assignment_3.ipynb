{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO_5731_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adEmybUfE7j0",
        "outputId": "0c8082b9-fff0-4f02-b92e-410fb90aa471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%tensorflow_version 1.8\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.8`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z92HTMx6FAZ0"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "name = [ ]\n",
        "name_List = [ ]\n",
        "abstract = [ ]\n",
        "abstract_List=[ ]\n",
        "page_number = [0,10,20,30,40,50,60,70,80,90]\n",
        "for i in page_number:\n",
        " page = requests.get('https://citeseerx.ist.psu.edu/search;jsessionid=87FF6C66EA09F22314C131669600CF98?q=natural+language+processing&t=doc&sort=rlv&start='+ str (i))\n",
        " soup = BeautifulSoup(page.content,'html.parser')\n",
        " name.append(soup.find_all('a',class_='remove doc_details'))\n",
        " abstract.append(soup.find_all('div',class_='pubabstract'))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azkrd8aTFJof"
      },
      "source": [
        "for j in name:\n",
        "  for k in range(len(j)):\n",
        "    name_List.append(j[k].get_text())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ViThIXjFKfN"
      },
      "source": [
        "for l in abstract:\n",
        "  for m in range(len(l)):\n",
        "    abstract_List.append(l[m].get_text())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR2kXvaqFOBX"
      },
      "source": [
        "df = pd.DataFrame({'Article':name_List,'Abstract': abstract_List})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ7yJDXsFQrj",
        "outputId": "7fab3f3a-daa0-4989-fe6d-40890871853c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n                  Foundations of statistical...</td>\n",
              "      <td>\\n                    Abstract not found\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n                  A Maximum Entropy approach...</td>\n",
              "      <td>\\n                     describe a method for s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nNatural Language Processing\\n</td>\n",
              "      <td>\\n                    Scaling conditional rand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n                  Linguistics and Natural La...</td>\n",
              "      <td>\\n                    The paper addresses the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\nNatural Language Processing\\n</td>\n",
              "      <td>\\n                    In most natural language...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Article                                           Abstract\n",
              "0  \\n                  Foundations of statistical...  \\n                    Abstract not found\\n    ...\n",
              "1  \\n                  A Maximum Entropy approach...  \\n                     describe a method for s...\n",
              "2                    \\nNatural Language Processing\\n  \\n                    Scaling conditional rand...\n",
              "3  \\n                  Linguistics and Natural La...  \\n                    The paper addresses the ...\n",
              "4                    \\nNatural Language Processing\\n  \\n                    In most natural language..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujQu_GTSFTC2",
        "outputId": "768292aa-7dca-4371-cd3e-9fc7710500cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#1 Noise removal\n",
        "df['Article'] = df['Article'].replace('\\n','',regex = True)\n",
        "df['Abstract'] = df['Abstract'].replace('\\n','',regex = True)\n",
        "df['Article'] = df['Article'].replace('[^\\w\\s]','',regex = True)\n",
        "df['Abstract'] = df['Abstract'].replace('[^\\w\\s]','',regex = True)\n",
        "#2 Remove numbers\n",
        "df['Article'] = df['Article'].str.replace('\\d+','')\n",
        "df['Abstract'] = df['Abstract'].str.replace('\\d+','')\n",
        "#3 Remove StopWords\n",
        "stop_Words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \n",
        "              \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\",\n",
        "              \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\",\n",
        "              \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\",\n",
        "              \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "              \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\",\n",
        "              \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n",
        "              \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n",
        "              \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\",\n",
        "              \"about\", \"against\", \"between\", \"into\", \"through\",\n",
        "              \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
        "              \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
        "              \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "              \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
        "              \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
        "              \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "              \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
        "              \"don\", \"should\", \"now\"]\n",
        "# stop = stopwords.words('english')\n",
        "df['Article'] = df['Article'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_Words))\n",
        "df['Abstract'] = df['Abstract'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_Words))\n",
        "#4 Lowercase all text\n",
        "df['Article'] = df['Article'].str.lower()\n",
        "df['Abstract'] = df['Abstract'].str.lower()\n",
        "#5 Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['Article'] = df['Article'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "df['Abstract']= df['Abstract'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "#6 Lemmatization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "df['Abstract']=df['Abstract'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdwacxuyHjW7",
        "outputId": "f044d246-a06f-4220-b426-b50326091218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>foundat statist natur languag process</td>\n",
              "      <td>abstract found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a maximum entropi approach natur languag process</td>\n",
              "      <td>describ method statist model base maximum entr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>natur languag process</td>\n",
              "      <td>scale condit random field natur languag proces...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>linguist natur languag process</td>\n",
              "      <td>the paper address issu cooper linguist natur l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>natur languag process</td>\n",
              "      <td>in natur languag process applic descript logic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>natur languag process almost scratch</td>\n",
              "      <td>we propos unifi neural network architectur lea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>natur languag process</td>\n",
              "      <td>natur languag process the subject natur langua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>natur languag processingrobot</td>\n",
              "      <td>robot interact human facetofac use natur langu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tutori natur languag process</td>\n",
              "      <td>natur languag languag spoken human current yet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ambigu natur languag process</td>\n",
              "      <td>abstract ambigu refer abil one mean understood...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Article                                           Abstract\n",
              "0             foundat statist natur languag process                                     abstract found\n",
              "1  a maximum entropi approach natur languag process  describ method statist model base maximum entr...\n",
              "2                             natur languag process  scale condit random field natur languag proces...\n",
              "3                    linguist natur languag process  the paper address issu cooper linguist natur l...\n",
              "4                             natur languag process  in natur languag process applic descript logic...\n",
              "5              natur languag process almost scratch  we propos unifi neural network architectur lea...\n",
              "6                             natur languag process  natur languag process the subject natur langua...\n",
              "7                     natur languag processingrobot  robot interact human facetofac use natur langu...\n",
              "8                      tutori natur languag process  natur languag languag spoken human current yet...\n",
              "9                      ambigu natur languag process  abstract ambigu refer abil one mean understood..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6oqbJqWXlIv",
        "outputId": "0c3ade1c-29c1-4a25-85f7-8e4bae792604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvE-5JFsKeLW",
        "outputId": "84625007-c65d-4042-f04f-23615a26497c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "l = [ ]\n",
        "trigrams = [ ]\n",
        "for i in df['Abstract']:\n",
        " l.append(nltk.word_tokenize(i))\n",
        "for j in l:\n",
        "  trigrams.append(ngrams(j,3))\n",
        "trigrams"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<generator object ngrams at 0x7feb5b5983b8>,\n",
              " <generator object ngrams at 0x7feb5b5989e8>,\n",
              " <generator object ngrams at 0x7feb5b598a40>,\n",
              " <generator object ngrams at 0x7feb5b598468>,\n",
              " <generator object ngrams at 0x7feb5b598990>,\n",
              " <generator object ngrams at 0x7feb5b598938>,\n",
              " <generator object ngrams at 0x7feb5b5988e0>,\n",
              " <generator object ngrams at 0x7feb5b598888>,\n",
              " <generator object ngrams at 0x7feb5b598830>,\n",
              " <generator object ngrams at 0x7feb5b598360>,\n",
              " <generator object ngrams at 0x7feb5b598570>,\n",
              " <generator object ngrams at 0x7feb5b598780>,\n",
              " <generator object ngrams at 0x7feb5b598410>,\n",
              " <generator object ngrams at 0x7feb5b598af0>,\n",
              " <generator object ngrams at 0x7feb5b598b48>,\n",
              " <generator object ngrams at 0x7feb5b598ba0>,\n",
              " <generator object ngrams at 0x7feb5b598bf8>,\n",
              " <generator object ngrams at 0x7feb5b598c50>,\n",
              " <generator object ngrams at 0x7feb5b598ca8>,\n",
              " <generator object ngrams at 0x7feb5b598d00>,\n",
              " <generator object ngrams at 0x7feb5b598d58>,\n",
              " <generator object ngrams at 0x7feb5b598db0>,\n",
              " <generator object ngrams at 0x7feb5b598e08>,\n",
              " <generator object ngrams at 0x7feb5b598e60>,\n",
              " <generator object ngrams at 0x7feb5b598eb8>,\n",
              " <generator object ngrams at 0x7feb5b598f10>,\n",
              " <generator object ngrams at 0x7feb5b598f68>,\n",
              " <generator object ngrams at 0x7feb5b598fc0>,\n",
              " <generator object ngrams at 0x7feb5b573048>,\n",
              " <generator object ngrams at 0x7feb5b5730a0>,\n",
              " <generator object ngrams at 0x7feb5b5730f8>,\n",
              " <generator object ngrams at 0x7feb5b573150>,\n",
              " <generator object ngrams at 0x7feb5b5731a8>,\n",
              " <generator object ngrams at 0x7feb5b573200>,\n",
              " <generator object ngrams at 0x7feb5b573258>,\n",
              " <generator object ngrams at 0x7feb5b5732b0>,\n",
              " <generator object ngrams at 0x7feb5b573308>,\n",
              " <generator object ngrams at 0x7feb5b573360>,\n",
              " <generator object ngrams at 0x7feb5b5733b8>,\n",
              " <generator object ngrams at 0x7feb5b573410>,\n",
              " <generator object ngrams at 0x7feb5b573468>,\n",
              " <generator object ngrams at 0x7feb5b5734c0>,\n",
              " <generator object ngrams at 0x7feb5b573518>,\n",
              " <generator object ngrams at 0x7feb5b573570>,\n",
              " <generator object ngrams at 0x7feb5b5735c8>,\n",
              " <generator object ngrams at 0x7feb5b573620>,\n",
              " <generator object ngrams at 0x7feb5b573678>,\n",
              " <generator object ngrams at 0x7feb5b5736d0>,\n",
              " <generator object ngrams at 0x7feb5b573728>,\n",
              " <generator object ngrams at 0x7feb5b573780>,\n",
              " <generator object ngrams at 0x7feb5b5737d8>,\n",
              " <generator object ngrams at 0x7feb5b573830>,\n",
              " <generator object ngrams at 0x7feb5b573888>,\n",
              " <generator object ngrams at 0x7feb5b5738e0>,\n",
              " <generator object ngrams at 0x7feb5b573938>,\n",
              " <generator object ngrams at 0x7feb5b573990>,\n",
              " <generator object ngrams at 0x7feb5b5739e8>,\n",
              " <generator object ngrams at 0x7feb5b573a40>,\n",
              " <generator object ngrams at 0x7feb5b573a98>,\n",
              " <generator object ngrams at 0x7feb5b573af0>,\n",
              " <generator object ngrams at 0x7feb5b573b48>,\n",
              " <generator object ngrams at 0x7feb5b573ba0>,\n",
              " <generator object ngrams at 0x7feb5b573bf8>,\n",
              " <generator object ngrams at 0x7feb5b573c50>,\n",
              " <generator object ngrams at 0x7feb5b573ca8>,\n",
              " <generator object ngrams at 0x7feb5b573d00>,\n",
              " <generator object ngrams at 0x7feb5b573d58>,\n",
              " <generator object ngrams at 0x7feb5b573db0>,\n",
              " <generator object ngrams at 0x7feb5b573e08>,\n",
              " <generator object ngrams at 0x7feb5b573e60>,\n",
              " <generator object ngrams at 0x7feb5b573eb8>,\n",
              " <generator object ngrams at 0x7feb5b573f10>,\n",
              " <generator object ngrams at 0x7feb5b573f68>,\n",
              " <generator object ngrams at 0x7feb5b573fc0>,\n",
              " <generator object ngrams at 0x7feb5b5aa048>,\n",
              " <generator object ngrams at 0x7feb5b5aa0a0>,\n",
              " <generator object ngrams at 0x7feb5b5aa0f8>,\n",
              " <generator object ngrams at 0x7feb5b5aa150>,\n",
              " <generator object ngrams at 0x7feb5b5aa1a8>,\n",
              " <generator object ngrams at 0x7feb5b5aa200>,\n",
              " <generator object ngrams at 0x7feb5b5aa258>,\n",
              " <generator object ngrams at 0x7feb5b5aa2b0>,\n",
              " <generator object ngrams at 0x7feb5b5aa308>,\n",
              " <generator object ngrams at 0x7feb5b5aa360>,\n",
              " <generator object ngrams at 0x7feb5b5aa3b8>,\n",
              " <generator object ngrams at 0x7feb5b5aa410>,\n",
              " <generator object ngrams at 0x7feb5b5aa468>,\n",
              " <generator object ngrams at 0x7feb5b5aa4c0>,\n",
              " <generator object ngrams at 0x7feb5b5aa518>,\n",
              " <generator object ngrams at 0x7feb5b5aa570>,\n",
              " <generator object ngrams at 0x7feb5b5aa5c8>,\n",
              " <generator object ngrams at 0x7feb5b5aa620>,\n",
              " <generator object ngrams at 0x7feb5b5aa678>,\n",
              " <generator object ngrams at 0x7feb5b5aa6d0>,\n",
              " <generator object ngrams at 0x7feb5b5aa728>,\n",
              " <generator object ngrams at 0x7feb5b5aa780>,\n",
              " <generator object ngrams at 0x7feb5b5aa7d8>,\n",
              " <generator object ngrams at 0x7feb5b5aa830>,\n",
              " <generator object ngrams at 0x7feb5b5aa888>,\n",
              " <generator object ngrams at 0x7feb5b5aa8e0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}