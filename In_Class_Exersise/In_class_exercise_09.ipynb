{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_09.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prithvikolla/Compuational_Methods_INFO5731/blob/master/In_Class_Exersise/In_class_exercise_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Wr9XQywCnS"
      },
      "source": [
        "# **The ninth in-class-exercise (20 points in total, 11/11/2020)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEF_KSDFwCnU"
      },
      "source": [
        "The purpose of the exercise is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from here: https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/exercise09_datacollection.zip. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7LLsbTXwCnY"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXSB82jy30VH",
        "outputId": "44801c13-7c47-4bd0-e6db-978ae33eb725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQF9EcAwWbJe"
      },
      "source": [
        "**LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjs8ZV3avEOi",
        "outputId": "5d88c159-4ffd-44ae-c1ff-33865bff10d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Train Data \n",
        "df_train = pd.read_csv(r'/content/stsa-train.txt',sep = 'delimiter',header= None)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC67pou7yOuJ"
      },
      "source": [
        "df_train.columns = ['Noise_text']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPEGQe0x2Ba8",
        "outputId": "70a86423-4b84-4c20-82fe-ad24c74eb1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Noise_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0 apparently reassembled from the cutting-room floor of any given daytime soap .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0 they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 this is a visually stunning rumination on love , memory , history and the war between art and commerce .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                            Noise_text\n",
              "0  1 a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films                                                                                                                          \n",
              "1  0 apparently reassembled from the cutting-room floor of any given daytime soap .                                                                                                                                                   \n",
              "2  0 they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
              "3  1 this is a visually stunning rumination on love , memory , history and the war between art and commerce .                                                                                                                         \n",
              "4  1 jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .                                                                                                                              "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC5sjvQP2mXU"
      },
      "source": [
        "df_train['Noise_text_split'] = df_train['Noise_text'].apply(lambda x: x.split())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbCyI-Pi4M0u",
        "outputId": "b21f369d-5d28-4ae4-b851-20c60f24d44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Noise_text</th>\n",
              "      <th>Noise_text_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films</td>\n",
              "      <td>[1, a, stirring, ,, funny, and, finally, transporting, re-imagining, of, beauty, and, the, beast, and, 1930s, horror, films]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0 apparently reassembled from the cutting-room floor of any given daytime soap .</td>\n",
              "      <td>[0, apparently, reassembled, from, the, cutting-room, floor, of, any, given, daytime, soap, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0 they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .</td>\n",
              "      <td>[0, they, presume, their, audience, wo, n't, sit, still, for, a, sociology, lesson, ,, however, entertainingly, presented, ,, so, they, trot, out, the, conventional, science-fiction, elements, of, bug-eyed, monsters, and, futuristic, women, in, skimpy, clothes, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 this is a visually stunning rumination on love , memory , history and the war between art and commerce .</td>\n",
              "      <td>[1, this, is, a, visually, stunning, rumination, on, love, ,, memory, ,, history, and, the, war, between, art, and, commerce, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .</td>\n",
              "      <td>[1, jonathan, parker, 's, bartleby, should, have, been, the, be-all-end-all, of, the, modern-office, anomie, films, .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                            Noise_text                                                                                                                                                                                                                                                          Noise_text_split\n",
              "0  1 a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films                                                                                                                            [1, a, stirring, ,, funny, and, finally, transporting, re-imagining, of, beauty, and, the, beast, and, 1930s, horror, films]                                                                                                                                            \n",
              "1  0 apparently reassembled from the cutting-room floor of any given daytime soap .                                                                                                                                                     [0, apparently, reassembled, from, the, cutting-room, floor, of, any, given, daytime, soap, .]                                                                                                                                                                          \n",
              "2  0 they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .  [0, they, presume, their, audience, wo, n't, sit, still, for, a, sociology, lesson, ,, however, entertainingly, presented, ,, so, they, trot, out, the, conventional, science-fiction, elements, of, bug-eyed, monsters, and, futuristic, women, in, skimpy, clothes, .]\n",
              "3  1 this is a visually stunning rumination on love , memory , history and the war between art and commerce .                                                                                                                           [1, this, is, a, visually, stunning, rumination, on, love, ,, memory, ,, history, and, the, war, between, art, and, commerce, .]                                                                                                                                        \n",
              "4  1 jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .                                                                                                                                [1, jonathan, parker, 's, bartleby, should, have, been, the, be-all-end-all, of, the, modern-office, anomie, films, .]                                                                                                                                                  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtTmSVDL4N8j"
      },
      "source": [
        "df_train['sentiment_train'] = df_train['Noise_text_split'].apply(lambda x: x[0])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e7sRFUIUjXr",
        "outputId": "82dab481-d33e-4fa1-f1cc-96b2fb49b1ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train['sentiment_train'].head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    0\n",
              "2    0\n",
              "3    1\n",
              "4    1\n",
              "Name: sentiment_train, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YezaQ402UllE"
      },
      "source": [
        "df_train['reviews_train'] = df_train['Noise_text_split'].apply(lambda x: ' '.join(x[1:]))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoitocHWU3Z5",
        "outputId": "a73695b4-6aa2-484f-fc2f-41ec344fb90b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train['reviews_train'].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films                                                                                                                          \n",
              "1    apparently reassembled from the cutting-room floor of any given daytime soap .                                                                                                                                                   \n",
              "2    they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
              "3    this is a visually stunning rumination on love , memory , history and the war between art and commerce .                                                                                                                         \n",
              "4    jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .                                                                                                                              \n",
              "Name: reviews_train, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Qy1wtIU8d6",
        "outputId": "2fe552e0-8a3f-46a4-b1ed-9623c439b2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "#Test Data\n",
        "df_test = pd.read_csv(r'/content/stsa-test.txt',sep = 'delimiter',header= None)\n",
        "df_test.columns = ['Noise_text']\n",
        "df_test['Noise_text_split'] = df_test['Noise_text'].apply(lambda x: x.split())\n",
        "df_test['sentiment_test'] = df_test['Noise_text_split'].apply(lambda x: x[0])\n",
        "df_test['reviews_test'] = df_test['Noise_text_split'].apply(lambda x: ' '.join(x[1:]))\n",
        "df_test.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Noise_text</th>\n",
              "      <th>Noise_text_split</th>\n",
              "      <th>sentiment_test</th>\n",
              "      <th>reviews_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0 no movement , no yuks , not much of anything .</td>\n",
              "      <td>[0, no, movement, ,, no, yuks, ,, not, much, of, anything, .]</td>\n",
              "      <td>0</td>\n",
              "      <td>no movement , no yuks , not much of anything .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0 a gob of drivel so sickly sweet , even the eager consumers of moore 's pasteurized ditties will retch it up like rancid crème brûlée .</td>\n",
              "      <td>[0, a, gob, of, drivel, so, sickly, sweet, ,, even, the, eager, consumers, of, moore, 's, pasteurized, ditties, will, retch, it, up, like, rancid, crème, brûlée, .]</td>\n",
              "      <td>0</td>\n",
              "      <td>a gob of drivel so sickly sweet , even the eager consumers of moore 's pasteurized ditties will retch it up like rancid crème brûlée .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0 gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up .</td>\n",
              "      <td>[0, gangs, of, new, york, is, an, unapologetic, mess, ,, whose, only, saving, grace, is, that, it, ends, by, blowing, just, about, everything, up, .]</td>\n",
              "      <td>0</td>\n",
              "      <td>gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0 we never really feel involved with the story , as all of its ideas remain just that : abstract ideas .</td>\n",
              "      <td>[0, we, never, really, feel, involved, with, the, story, ,, as, all, of, its, ideas, remain, just, that, :, abstract, ideas, .]</td>\n",
              "      <td>0</td>\n",
              "      <td>we never really feel involved with the story , as all of its ideas remain just that : abstract ideas .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 this is one of polanski 's best films .</td>\n",
              "      <td>[1, this, is, one, of, polanski, 's, best, films, .]</td>\n",
              "      <td>1</td>\n",
              "      <td>this is one of polanski 's best films .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                 Noise_text  ...                                                                                                                            reviews_test\n",
              "0  0 no movement , no yuks , not much of anything .                                                                                          ...  no movement , no yuks , not much of anything .                                                                                        \n",
              "1  0 a gob of drivel so sickly sweet , even the eager consumers of moore 's pasteurized ditties will retch it up like rancid crème brûlée .  ...  a gob of drivel so sickly sweet , even the eager consumers of moore 's pasteurized ditties will retch it up like rancid crème brûlée .\n",
              "2  0 gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up .               ...  gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up .             \n",
              "3  0 we never really feel involved with the story , as all of its ideas remain just that : abstract ideas .                                  ...  we never really feel involved with the story , as all of its ideas remain just that : abstract ideas .                                \n",
              "4  1 this is one of polanski 's best films .                                                                                                 ...  this is one of polanski 's best films .                                                                                               \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xddyKw3qWiaj"
      },
      "source": [
        "**DATA CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKh6aTRSWQHg",
        "outputId": "1dc60684-f1a9-499e-fba8-3b32f32b3a81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3MkXDucXOW7"
      },
      "source": [
        "df_train['reviews_train'] = df_train['reviews_train'].apply(lambda x: x.lower())\n",
        "df_test['reviews_test'] = df_test['reviews_test'].apply(lambda x: x.lower())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB4RA7KuXmhU"
      },
      "source": [
        "df_train['reviews_train_rpunc'] = df_train['reviews_train'].str.replace('[^\\w\\s]','')\n",
        "df_test['reviews_test_rpunc'] = df_test['reviews_test'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9uTw3xmYSuG",
        "outputId": "0e9eca33-327f-4e0c-e2aa-4543f05beb8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WpbuwiYxuK"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyX9rFIvY7c2"
      },
      "source": [
        "#train\n",
        "df_train['reviews_token'] = df_train['reviews_train_rpunc'].apply(lambda x: word_tokenize(x))\n",
        "df_train['reviews_stop'] = df_train['reviews_token'].apply(lambda x: [i for i in x if i not in stopword])\n",
        "df_train['reviews_train_lemma'] = df_train['reviews_stop'].apply(lambda x: [wnl.lemmatize(i) for i in x])\n",
        "df_train['reviews_train_lemma'] = df_train['reviews_train_lemma'].apply(lambda x: ' '.join(map(str, x)) )\n",
        "#test\n",
        "df_test['reviews_token'] = df_test['reviews_test_rpunc'].apply(lambda x: word_tokenize(x))\n",
        "df_test['reviews_stop'] = df_test['reviews_token'].apply(lambda x: [i for i in x if i not in stopword])\n",
        "df_test['reviews_test_lemma'] = df_test['reviews_stop'].apply(lambda x: [wnl.lemmatize(i) for i in x])\n",
        "df_test['reviews_test_lemma'] = df_test['reviews_test_lemma'].apply(lambda x: ' '.join(map(str, x)) )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63eDtfaaZrD"
      },
      "source": [
        "Stemming takes away the semantics of the words, instead of stemming we are using Lemmatization here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVBKb122aKcC"
      },
      "source": [
        "#Making a copy of Data Sets\n",
        "df_train_copy = df_train.copy()\n",
        "df_test_copy = df_test.copy()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGu5NbrfaNyd"
      },
      "source": [
        "df_train = df_train[['reviews_train_lemma','sentiment_train']]\n",
        "df_test = df_test[['reviews_test_lemma','sentiment_test']]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GxFOIvtca-0"
      },
      "source": [
        "**DATA TRANSFORMATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mcsh0RcbXpY"
      },
      "source": [
        "#TFIDF Vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "#train\n",
        "X_tfidf_train = tfidf_vect.fit_transform(df_train['reviews_train_lemma'])\n",
        "#test\n",
        "X_tfidf_test = tfidf_vect.transform(df_test['reviews_test_lemma'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MDmExv7dZGz"
      },
      "source": [
        "**MACHINE LEARNING:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrzOfIw8bgRm"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNbQgaoF1BAL"
      },
      "source": [
        "**TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRHdIVIpfK8m",
        "outputId": "8729d974-5a5f-4c4a-f23e-847a6ef28bd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = X_tfidf_train\n",
        "y = df_train['sentiment_train']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 62.57%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.37      0.49       675\n",
            "           1       0.59      0.87      0.70       709\n",
            "\n",
            "    accuracy                           0.63      1384\n",
            "   macro avg       0.66      0.62      0.60      1384\n",
            "weighted avg       0.66      0.63      0.60      1384\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT7Vt7_crxPf",
        "outputId": "eb446835-5dde-4ae5-9143-2f4702df09de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('XGBoost:',np.mean(cross_val_score(model, X_test, y_test, cv=10)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoost: 0.6112761964341571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EVEGud0wDtX",
        "outputId": "5647415b-5997-4214-ee03-477b8a32a919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model1 = MultinomialNB()\n",
        "model1.fit(X_train,y_train)\n",
        "y_pred = model1.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 76.95%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.68      0.74       675\n",
            "           1       0.74      0.85      0.79       709\n",
            "\n",
            "    accuracy                           0.77      1384\n",
            "   macro avg       0.78      0.77      0.77      1384\n",
            "weighted avg       0.78      0.77      0.77      1384\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nytMjL20yoc",
        "outputId": "d197a60e-20e5-4e3b-f89c-9fd1497d84a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Naive_bayes:',np.mean(cross_val_score(model1, X_test, y_test, cv=10)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive_bayes: 0.7152330309665311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPlah-tG0rkD",
        "outputId": "bdd5bc98-2ff2-4df8-cf02-1e15583142ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model2 = LinearSVC()\n",
        "model2.fit(X_train,y_train)\n",
        "y_pred = model2.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 77.02%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.76      0.76       675\n",
            "           1       0.77      0.78      0.78       709\n",
            "\n",
            "    accuracy                           0.77      1384\n",
            "   macro avg       0.77      0.77      0.77      1384\n",
            "weighted avg       0.77      0.77      0.77      1384\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uRk2NQl01zq",
        "outputId": "78eeae94-67cd-4edf-85e3-2a6ab4fa84ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('SVM:',np.mean(cross_val_score(model2, X_test, y_test, cv=10)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM: 0.7101657804191429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpqXmq3_0u47",
        "outputId": "ba97f0b0-45b7-4c67-bbf5-2cd84ab86cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model3 = KNeighborsClassifier(n_neighbors=5)\n",
        "model3.fit(X_train,y_train)\n",
        "y_pred = model3.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 49.64%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.99      0.66       675\n",
            "           1       0.77      0.02      0.05       709\n",
            "\n",
            "    accuracy                           0.50      1384\n",
            "   macro avg       0.63      0.51      0.35      1384\n",
            "weighted avg       0.64      0.50      0.34      1384\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgiOSr2_04kf",
        "outputId": "d8aca9a5-0ae8-44a3-d83f-bdd888114d71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('KNN:',np.mean(cross_val_score(model3, X_test, y_test, cv=10)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN: 0.6538734230007299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKK1S_j70vuX",
        "outputId": "49736bea-807f-44f2-ed13-cd9da5ad08aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model4 = DecisionTreeClassifier()\n",
        "model4.fit(X_train,y_train)\n",
        "y_pred = model4.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 65.68%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.65      0.65       675\n",
            "           1       0.67      0.66      0.66       709\n",
            "\n",
            "    accuracy                           0.66      1384\n",
            "   macro avg       0.66      0.66      0.66      1384\n",
            "weighted avg       0.66      0.66      0.66      1384\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU5FUmI305ap",
        "outputId": "37b6b486-f328-4968-da8c-b01a605ad6a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Decision Tree:',np.mean(cross_val_score(model4, X_test, y_test, cv=10)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree: 0.6083463663851527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC2yCVPl0whJ",
        "outputId": "3cfff973-2c75-4d77-e416-a3a0c097e1ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model5 = RandomForestClassifier()\n",
        "model5.fit(X_train,y_train)\n",
        "y_pred = model5.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 71.03%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.71      0.70       675\n",
            "           1       0.72      0.71      0.72       709\n",
            "\n",
            "    accuracy                           0.71      1384\n",
            "   macro avg       0.71      0.71      0.71      1384\n",
            "weighted avg       0.71      0.71      0.71      1384\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9AX2TPa06JW",
        "outputId": "c714c079-f1f0-4a6a-9695-9ce2470d6da3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Random Forrest:',np.mean(cross_val_score(model5, X_test, y_test, cv=10)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forrest: 0.6466948180585966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJeHfPr-07Id"
      },
      "source": [
        "**TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAupdj1O0_e8",
        "outputId": "63ed6ff3-eece-4460-abd6-966bdd2aad0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#XGBoost\n",
        "test_pred = model.predict(X_tfidf_test)\n",
        "accuracy = accuracy_score(df_test['sentiment_test'], test_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(df_test['sentiment_test'],test_pred))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 63.65%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.40      0.52       912\n",
            "           1       0.59      0.87      0.71       909\n",
            "\n",
            "    accuracy                           0.64      1821\n",
            "   macro avg       0.68      0.64      0.62      1821\n",
            "weighted avg       0.68      0.64      0.61      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFPe-UyB3N33",
        "outputId": "e58e39ce-620f-4772-e8cc-1b46a8fb97a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Naive_bayes\n",
        "y_pred = model1.predict(X_tfidf_test)\n",
        "accuracy = accuracy_score(df_test['sentiment_test'], y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(df_test['sentiment_test'],y_pred))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 78.58%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.69      0.76       912\n",
            "           1       0.74      0.89      0.81       909\n",
            "\n",
            "    accuracy                           0.79      1821\n",
            "   macro avg       0.80      0.79      0.78      1821\n",
            "weighted avg       0.80      0.79      0.78      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhM1J9Fs3N6g",
        "outputId": "2354e896-6ec8-4aae-ccf0-2122e681e774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#SVM\n",
        "y_pred = model2.predict(X_tfidf_test)\n",
        "accuracy = accuracy_score(df_test['sentiment_test'], y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(df_test['sentiment_test'],y_pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 77.98%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.75      0.77       912\n",
            "           1       0.76      0.81      0.79       909\n",
            "\n",
            "    accuracy                           0.78      1821\n",
            "   macro avg       0.78      0.78      0.78      1821\n",
            "weighted avg       0.78      0.78      0.78      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caSD_gC03N89",
        "outputId": "1f92fb8d-6eb9-43b7-90a3-a8d72ee5a9f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#KNN\n",
        "y_pred = model3.predict(X_tfidf_test)\n",
        "accuracy = accuracy_score(df_test['sentiment_test'], y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(df_test['sentiment_test'],y_pred))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 51.24%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.99      0.67       912\n",
            "           1       0.82      0.03      0.06       909\n",
            "\n",
            "    accuracy                           0.51      1821\n",
            "   macro avg       0.66      0.51      0.36      1821\n",
            "weighted avg       0.66      0.51      0.36      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6ilbBLv3N_c",
        "outputId": "8a2ed237-6824-4ac9-e57b-fe60465edaac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Decision Tree\n",
        "y_pred = model4.predict(X_tfidf_test)\n",
        "accuracy = accuracy_score(df_test['sentiment_test'], y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(df_test['sentiment_test'],y_pred))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 65.24%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.68      0.66       912\n",
            "           1       0.66      0.63      0.64       909\n",
            "\n",
            "    accuracy                           0.65      1821\n",
            "   macro avg       0.65      0.65      0.65      1821\n",
            "weighted avg       0.65      0.65      0.65      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FEul4Oj3NzM",
        "outputId": "acc9532f-f49a-419b-b912-55bc1f998c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Random Forrest\n",
        "y_pred = model5.predict(X_tfidf_test)\n",
        "accuracy = accuracy_score(df_test['sentiment_test'], y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(classification_report(df_test['sentiment_test'],y_pred))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 73.48%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.74      0.74       912\n",
            "           1       0.74      0.73      0.73       909\n",
            "\n",
            "    accuracy                           0.73      1821\n",
            "   macro avg       0.73      0.73      0.73      1821\n",
            "weighted avg       0.73      0.73      0.73      1821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gfl8PBm4uww"
      },
      "source": [
        "**RESULT:**\n",
        "\n",
        "We see from the abve accuracies on test data that MultinomialNB and SVM has highest accuracies among all other algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nQyDu3TOiBD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}