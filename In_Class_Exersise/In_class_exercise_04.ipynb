{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04_Kolla.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vR0L3_CreM_A",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import io\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcIACRls0o0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "465f5066-49c4-46f2-8317-0d0d50f37378"
      },
      "source": [
        "urlfile=\"https://raw.githubusercontent.com/unt-iialab/INFO5731_FALL2020/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt\"\n",
        "\n",
        "url=requests.get(urlfile).content\n",
        "\n",
        "cols = ['text']\n",
        "\n",
        "df =pd.read_csv(io.StringIO(url.decode('utf-8')), names= cols)\n",
        "\n",
        "df.head(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>June Term</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Synopsis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WRIT of Error to the Circuit Court of Sumter.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>West Headnotes (2)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            text\n",
              "0                                     5 Ala. 740\n",
              "1                      Supreme Court of Alabama.\n",
              "2                                          ADAMS\n",
              "3                                             v.\n",
              "4                             TANNER AND HORTON.\n",
              "5                                      June Term\n",
              "6                                       Synopsis\n",
              "7  WRIT of Error to the Circuit Court of Sumter.\n",
              "8                                              Â \n",
              "9                             West Headnotes (2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBQlgF3-0p6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "90330481-6dfa-45df-d91a-94f27a6ff870"
      },
      "source": [
        "#1.1 Basic feature extraction using text data (4 points)\n",
        "# Remove null rows in column\n",
        "df['text'] = df['text'].dropna()\n",
        "#Number of sentences\n",
        "len(df['text'])\n",
        "# Number of words\n",
        "df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "df[['text','word_count']].head()\n",
        "# Number of characters\n",
        "df['char_count'] = df['text'].str.len() \n",
        "df[['text','char_count']].head()\n",
        "# Average word length\n",
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  try: \n",
        "      return (sum(len(word) for word in words)/len(words))\n",
        "  except ZeroDivisionError:\n",
        "     print (\"divide by zero\")\n",
        "\n",
        "df['avg_word'] = df['text'].apply(lambda x: avg_word(x))\n",
        "df[['text','avg_word']].head()\n",
        "# Number of stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "df['stopwords'] = df['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "df[['text','stopwords']].head()\n",
        "# Number of special characters\n",
        "df['hastags'] = df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('*')])) # as most of the data contains * starting in the words.\n",
        "df[['text','hastags']].head()\n",
        "# Number of numerics\n",
        "df['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "df[['text','numerics']].head()\n",
        "# Number of uppercase words\n",
        "df['upper'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "df[['text','upper']].head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "divide by zero\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        text  upper\n",
              "0                 5 Ala. 740      0\n",
              "1  Supreme Court of Alabama.      0\n",
              "2                      ADAMS      1\n",
              "3                         v.      0\n",
              "4         TANNER AND HORTON.      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvPBZsaQ6L_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "1508cad6-a72f-4a68-de8d-f59c6c20e5b6"
      },
      "source": [
        "# 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "# Lower casing\n",
        "df['lower'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df[['text','lower']].head()\n",
        "# Punctuation removal\n",
        "df['punctuation'] = df['text'].str.replace('[^\\w\\s]','')\n",
        "df[['text','punctuation']].head()\n",
        "# Stopwords removal\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['stopwords'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df[['text','stopwords']].head()\n",
        "# Frequent words removal\n",
        "\n",
        "freq = pd.Series(' '.join(df['text']).split()).value_counts()[:10] #top 10 frequently appearing\n",
        "freq = list(freq.index)\n",
        "freq\n",
        "df['rem_freq_text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "df[['text','rem_freq_text']].head()\n",
        "\n",
        "\n",
        "# Rare words removal\n",
        "\n",
        "freq_rare = pd.Series(' '.join(df['text']).split()).value_counts()[-10:]\n",
        "freq_rare \n",
        "freq_rare = list(freq_rare.index)\n",
        "df['rem_rare_text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_rare))\n",
        "df[['text','rem_rare_text']].head()\n",
        "# Spelling correction\n",
        "\n",
        "from textblob import TextBlob\n",
        "df['correct_spell']=df['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "df[['text','correct_spell']].head()\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "df['token_text'] = df.apply(lambda row: nltk.word_tokenize(str (row['text'])), axis=1)\n",
        "df[['text','token_text']].head()\n",
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['stem_text']=df['text'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "df[['text','stem_text']].head(10)\n",
        "# Lemmatization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df['lem_text'] = df['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "df[['text','lem_text']].head(20)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lem_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>5 Ala. 740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>ADAMS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>v.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>June Term</td>\n",
              "      <td>June Term</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Synopsis</td>\n",
              "      <td>Synopsis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WRIT of Error to the Circuit Court of Sumter.</td>\n",
              "      <td>WRIT of Error to the Circuit Court of Sumter.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>West Headnotes (2)</td>\n",
              "      <td>West Headnotes (2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[1]</td>\n",
              "      <td>[1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Chattel Mortgages</td>\n",
              "      <td>Chattel Mortgages</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Crops</td>\n",
              "      <td>Crops</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>A growing crop has such an existence as to be ...</td>\n",
              "      <td>A growing crop ha such an existence a to be th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4 Cases that cite this headnote</td>\n",
              "      <td>4 Cases that cite this headnote</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[2]</td>\n",
              "      <td>[2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Creditorsâ Remedies</td>\n",
              "      <td>Creditorsâ Remedies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Lien and Priority</td>\n",
              "      <td>Lien and Priority</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Under St.1821</td>\n",
              "      <td>Under St.1821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5 Cases that cite this headnote</td>\n",
              "      <td>5 Cases that cite this headnote</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text                                           lem_text\n",
              "0                                          5 Ala. 740                                         5 Ala. 740\n",
              "1                           Supreme Court of Alabama.                          Supreme Court of Alabama.\n",
              "2                                               ADAMS                                              ADAMS\n",
              "3                                                  v.                                                 v.\n",
              "4                                  TANNER AND HORTON.                                 TANNER AND HORTON.\n",
              "5                                           June Term                                          June Term\n",
              "6                                            Synopsis                                           Synopsis\n",
              "7       WRIT of Error to the Circuit Court of Sumter.      WRIT of Error to the Circuit Court of Sumter.\n",
              "8                                                   Â                                                    \n",
              "9                                  West Headnotes (2)                                 West Headnotes (2)\n",
              "10                                                [1]                                                [1]\n",
              "11                                  Chattel Mortgages                                  Chattel Mortgages\n",
              "12                                              Crops                                              Crops\n",
              "13  A growing crop has such an existence as to be ...  A growing crop ha such an existence a to be th...\n",
              "14                    4 Cases that cite this headnote                    4 Cases that cite this headnote\n",
              "15                                                [2]                                                [2]\n",
              "16                                Creditorsâ Remedies                                Creditorsâ Remedies\n",
              "17                                  Lien and Priority                                  Lien and Priority\n",
              "18                                      Under St.1821                                      Under St.1821\n",
              "19                    5 Cases that cite this headnote                    5 Cases that cite this headnote"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFBIh9YyLGtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.3 Save all the clean sentences to a csv file (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "df.to_csv(\"legal_case.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlEE_b8TLdqY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "0196dcae-65e0-487b-9e3f-3a278ccb7a25"
      },
      "source": [
        "#1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "# Calculate the term frequency of all the terms.\n",
        "# Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n",
        "\n",
        "from nltk.util import ngrams\n",
        "eng_stopwords = set(stopwords.words('english'))\n",
        "def get_unigrams(sen):\n",
        "    return [word for word in nltk.word_tokenize(sen.lower()) if word not in eng_stopwords]\n",
        "df[\"unigrams\"] = df['text'].apply(lambda x: get_unigrams(str (x)))\n",
        "def get_bigrams(sen):\n",
        "    return [ i for i in ngrams(sen,2)]\n",
        "df[\"bigrams\"] = df[\"unigrams\"].apply(lambda x: get_bigrams(x))\n",
        "# df['trigrams'] = [i for i in ngrams(df['unigrams'], 3)]\n",
        "def get_trigrams(sen):\n",
        "    return [ i for i in ngrams(sen,3)]\n",
        "df[\"trigrams\"] = df[\"unigrams\"].apply(lambda x: get_trigrams(x))\n",
        "print(\"top 10 unigrams are:\\n\\n\",df['unigrams'].head(10))\n",
        "print(\"\\n\")\n",
        "print(\"top 10 bigrams are:\\n\\n\",df['bigrams'].head(10))\n",
        "print(\"\\n\")\n",
        "print(\"top 10 trigrams are:\\n\\n\",df['trigrams'].head(10))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 10 unigrams are:\n",
            "\n",
            " 0                              [5, ala., 740]\n",
            "1                [supreme, court, alabama, .]\n",
            "2                                     [adams]\n",
            "3                                      [v, .]\n",
            "4                         [tanner, horton, .]\n",
            "5                                [june, term]\n",
            "6                                  [synopsis]\n",
            "7    [writ, error, circuit, court, sumter, .]\n",
            "8                                          []\n",
            "9                  [west, headnotes, (, 2, )]\n",
            "Name: unigrams, dtype: object\n",
            "\n",
            "\n",
            "top 10 bigrams are:\n",
            "\n",
            " 0                             [(5, ala.), (ala., 740)]\n",
            "1    [(supreme, court), (court, alabama), (alabama,...\n",
            "2                                                   []\n",
            "3                                             [(v, .)]\n",
            "4                      [(tanner, horton), (horton, .)]\n",
            "5                                       [(june, term)]\n",
            "6                                                   []\n",
            "7    [(writ, error), (error, circuit), (circuit, co...\n",
            "8                                                   []\n",
            "9    [(west, headnotes), (headnotes, (), ((, 2), (2...\n",
            "Name: bigrams, dtype: object\n",
            "\n",
            "\n",
            "top 10 trigrams are:\n",
            "\n",
            " 0                                     [(5, ala., 740)]\n",
            "1     [(supreme, court, alabama), (court, alabama, .)]\n",
            "2                                                   []\n",
            "3                                                   []\n",
            "4                                [(tanner, horton, .)]\n",
            "5                                                   []\n",
            "6                                                   []\n",
            "7    [(writ, error, circuit), (error, circuit, cour...\n",
            "8                                                   []\n",
            "9    [(west, headnotes, (), (headnotes, (, 2), ((, ...\n",
            "Name: trigrams, dtype: object\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wSv6fVhOfFmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17250cac-adb7-416b-a580-486dc63b0ff4"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "def rem_lead_zero(o_ip):  \n",
        "    n_ip = \".\".join([str (int(i)) for i in o_ip.split(\".\")])   \n",
        "    return n_ip \n",
        "o_ip = \"260.08.094.109\"\n",
        "print(rem_lead_zero(o_ip))\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xdJpDx9gjbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15f8e7f5-3928-4a89-94e9-7e3c1f9464bd"
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "year = re.findall('(\\d{4})', sentence)\n",
        "year"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2010', '1000', '2010', '2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJF205L0iA5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}