{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO_5731_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adEmybUfE7j0",
        "outputId": "d62bdb52-2f9f-4217-94ef-4fd12e58505a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%tensorflow_version 1.8\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.8`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z92HTMx6FAZ0",
        "outputId": "7e19eccb-4da0-4292-f7ba-d6fee34dbc5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "name = [ ]\n",
        "name_List = [ ]\n",
        "abstract = [ ]\n",
        "abstract_List=[ ]\n",
        "page_number = [0,10,20,30,40,50,60,70,80,90]\n",
        "for i in page_number:\n",
        " page = requests.get('https://citeseerx.ist.psu.edu/search;jsessionid=87FF6C66EA09F22314C131669600CF98?q=natural+language+processing&t=doc&sort=rlv&start='+ str (i), verify=False)\n",
        " soup = BeautifulSoup(page.content,'html.parser')\n",
        " name.append(soup.find_all('a',class_='remove doc_details'))\n",
        " abstract.append(soup.find_all('div',class_='pubabstract'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azkrd8aTFJof"
      },
      "source": [
        "for j in name:\n",
        "  for k in range(len(j)):\n",
        "    name_List.append(j[k].get_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ViThIXjFKfN"
      },
      "source": [
        "for l in abstract:\n",
        "  for m in range(len(l)):\n",
        "    abstract_List.append(l[m].get_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR2kXvaqFOBX"
      },
      "source": [
        "df = pd.DataFrame({'Article':name_List,'Abstract': abstract_List})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ7yJDXsFQrj"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujQu_GTSFTC2"
      },
      "source": [
        "#1 Noise removal\n",
        "df['Article'] = df['Article'].replace('\\n','',regex = True)\n",
        "df['Abstract'] = df['Abstract'].replace('\\n','',regex = True)\n",
        "df['Article'] = df['Article'].replace('[^\\w\\s]','',regex = True)\n",
        "df['Abstract'] = df['Abstract'].replace('[^\\w\\s]','',regex = True)\n",
        "#2 Remove numbers\n",
        "df['Article'] = df['Article'].str.replace('\\d+','')\n",
        "df['Abstract'] = df['Abstract'].str.replace('\\d+','')\n",
        "#3 Remove StopWords\n",
        "stop_Words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \n",
        "              \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\",\n",
        "              \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\",\n",
        "              \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\",\n",
        "              \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "              \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\",\n",
        "              \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n",
        "              \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n",
        "              \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\",\n",
        "              \"about\", \"against\", \"between\", \"into\", \"through\",\n",
        "              \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
        "              \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
        "              \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "              \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
        "              \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
        "              \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "              \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
        "              \"don\", \"should\", \"now\"]\n",
        "# stop = stopwords.words('english')\n",
        "df['Article'] = df['Article'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_Words))\n",
        "df['Abstract'] = df['Abstract'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_Words))\n",
        "#4 Lowercase all text\n",
        "df['Article'] = df['Article'].str.lower()\n",
        "df['Abstract'] = df['Abstract'].str.lower()\n",
        "#5 Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['Article'] = df['Article'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "df['Abstract']= df['Abstract'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "#6 Lemmatization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "df['Abstract']=df['Abstract'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdwacxuyHjW7"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6oqbJqWXlIv"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvE-5JFsKeLW"
      },
      "source": [
        "df['WordToken_Abstract'] = df['Abstract'].apply(lambda x: word_tokenize(x))\n",
        "df['WordToken_Article'] = df['Article'].apply(lambda x: word_tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py1wjCCqxX6d"
      },
      "source": [
        "df['TriGram_Abstract'] = df['WordToken_Abstract'].apply(lambda x: list (ngrams(x,3)))\n",
        "df['TriGram_Article'] = df['WordToken_Article'].apply(lambda x: list (ngrams(x,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9jWd0pXxyko"
      },
      "source": [
        "df['TriGram_Abstract'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCL-ozg1MLI4"
      },
      "source": [
        "df['TriGram_Article'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDMSRT4zNrv6"
      },
      "source": [
        "article_Trigram_list = list (df['TriGram_Article'])\n",
        "abstract_Trigram_list = list (df['TriGram_Abstract'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiJm7NWvTqEt"
      },
      "source": [
        "article_Trigram_list[:3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}